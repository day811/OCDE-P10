id: test
namespace: company.team
description: "P10 Wine Sales ETL Pipeline with Professional Error Handling and Email Notification"

# ===== CONFIGURATION GLOBALE D'ERREURS (Namespace level) =====
errors:
  - id: error_summary
    type: io.kestra.plugin.core.log.Log
    message: |
      ðŸ”´ PIPELINE CRASHED
      Execution: {{ execution.id }}
      Flow: {{ flow.id }}
      Errors: {{ execution.failures }}

  - id: cleanup_temp
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - rm -rf {{ vars.tempDir }}/*

variables:
  sourceErp: /app/data/sources/Fichier_erp.xlsx
  sourceWeb: /app/data/sources/Fichier_web.xlsx
  sourceLink: /app/data/sources/fichier_liaison.xlsx
  tempDir: /app/data/tmp
  outputDir: /app/data/output
  emailError: "admin@company.com"

tasks:
  - id: load_files
    type: io.kestra.plugin.core.flow.Parallel
    description: "ETL Files in parallel with error handling"

    tasks:
      # =============== BRANCHE ERP ===============
      - id: process_erp
        type: io.kestra.plugin.core.flow.Sequential
        description: "ERP extract â†’ clean â†’ dedup â†’ validate"

        tasks:
          # 1. LOAD SOURCE
          - id: load_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: Extract ERP data from Excel
            retry:
              type: constant
              maxAttempts: 3
              maxDuration: PT5M
              interval: PT3M

            sql: |-
              -- total lignes source
              CREATE OR REPLACE TABLE erp_raw AS
              SELECT *
              FROM read_xlsx('{{ vars.sourceErp }}', header = true);

              COPY erp_raw TO '{{ vars.tempDir }}/erp_raw.parquet' (FORMAT PARQUET);

              SELECT COUNT(*) AS total_raw FROM erp_raw;
            fetchType: FETCH_ONE
            
          - id: clean_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: Remove rows with NULL product_id
            sql: |
              CREATE OR REPLACE TABLE erp_clean AS

              SELECT *
              FROM read_parquet('{{ vars.tempDir }}/erp_raw.parquet')
              WHERE product_id IS NOT NULL;

              COPY erp_clean TO '{{ vars.tempDir }}/erp_clean.parquet' (FORMAT PARQUET);

              SELECT total_clean, {{ outputs.load_erp.outputs[0].row.total_raw }} - total_clean AS cleaned FROM ( SELECT COUNT(*) AS total_clean FROM erp_clean)
            fetchType: FETCH_ONE

          # 3. DEDUP CLEANED SOURCE
          - id: dedup_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Remove duplicate product_id rows"

            sql: |-
              CREATE OR REPLACE TABLE dedup_erp AS
              SELECT DISTINCT ON(product_id) * 
              FROM read_parquet('{{ vars.tempDir }}/erp_clean.parquet');

              COPY dedup_erp TO '{{ vars.tempDir }}/dedup_erp.parquet' (FORMAT PARQUET);

              SELECT 
                COUNT(*) AS total_dedup,
                {{ outputs.clean_erp.outputs[0].row.total_clean }} - COUNT(*) AS dedup
              FROM dedup_erp;

            fetchType: FETCH_ONE

          # 4. TEST FINAL DATA DB
          - id: test_erp
            type: io.kestra.plugin.scripts.python.Script
            description: "Test ERP data quality metrics"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            dependencies:
              - duckdb
              - kestra

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            script: |
              import duckdb
              from kestra import Kestra

              dedup_file = "{{ vars.tempDir }}/dedup_erp.parquet"

              # Test 1: Count missing product_id
              sql = f"""
              SELECT COUNT(*) as total FROM read_parquet('{dedup_file}')
              WHERE product_id IS NULL;
              """
              result = duckdb.sql(sql).fetchall()
              missings = result[0][0]
              print(f"Missing product_id: {missings}")

              # Test 2: Count duplicate product_id
              sql = f"""
              SELECT COUNT(*) as nb_product_id_doublons
              FROM (
                SELECT product_id
                FROM read_parquet('{dedup_file}')
                GROUP BY product_id
                HAVING COUNT(*) > 1
              ) AS d
              """
              result = duckdb.sql(sql).fetchall()
              duplicated = result[0][0]
              print(f"Duplicated product_id: {duplicated}")

              Kestra.outputs({"missings": missings, "duplicated": duplicated})

          # 5. â­ CRITICAL: Validate ERP Quality
          - id: validate_erp_quality
            type: io.kestra.plugin.scripts.python.Script
            description: "â­ CRITICAL: Validate ERP quality - STOP pipeline if errors"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT30S
              maxAttempts: 2
              interval: PT10S

            script: |
              from kestra import Kestra

              missings = int("{{ outputs.test_erp.vars.missings }}")
              duplicated = int("{{ outputs.test_erp.vars.duplicated }}")

              print("=" * 60)
              print("ðŸ” ERP DATA QUALITY VALIDATION")
              print("=" * 60)
              print(f"  â€¢ Missing product_id: {missings}")
              print(f"  â€¢ Duplicated product_id: {duplicated}")

              MAX_MISSINGS = 0
              MAX_DUPLICATED = 0

              errors = []

              if missings > MAX_MISSINGS:
                  errors.append(f"CRITICAL: {missings} rows with missing product_id detected")

              if duplicated > MAX_DUPLICATED:
                  errors.append(f"CRITICAL: {duplicated} duplicate product_id values detected")

              if errors:
                  print("\n" + "=" * 60)
                  print("VALIDATION FAILED - STOPPING PIPELINE")
                  print("=" * 60)
                  for error in errors:
                      print(f"  {error}")
                  print("=" * 60)
                  raise Exception(f"ERP Quality Validation Failed: {len(errors)} critical issue(s)")

              print("\n" + "=" * 60)
              print("VALIDATION PASSED - ERP data quality OK")
              print("=" * 60)
              Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

          # 6. REPORT
          - id: log_erp
            type: io.kestra.plugin.core.log.Log
            message: |
              ðŸ“Š â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                       ERP PIPELINE SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ðŸ“ Source: Fichier_erp.xlsx
              ðŸ“ˆ Total rows loaded: {{ outputs.load_erp.outputs[0].row.total_raw }}
              ðŸ§¹ Rows cleaned: {{ outputs.clean_erp.outputs[0].row.cleaned }}
              ðŸ”„ Rows after dedup: {{ outputs.dedup_erp.outputs[0].row.total_dedup }}
              ðŸ§ª Missing product_id: {{ outputs.test_erp.vars.missings }}
              ðŸ§ª Duplicated product_id: {{ outputs.test_erp.vars.duplicated }}
              âœ… Quality check: {{ outputs.validate_erp_quality.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      # =============== BRANCHE WEB ===============
      - id: process_web
        type: io.kestra.plugin.core.flow.Sequential
        description: "WEB extract â†’ clean â†’ dedup â†’ validate"

        tasks:
          # 1. LOAD SOURCE
          - id: load_web
            type: io.kestra.plugin.scripts.python.Script
            description: "Load WEB data from Excel with retry"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            dependencies:
              - pandas
              - openpyxl
              - fastparquet
              - kestra

            retry:
              type: constant
              maxDuration: PT60S
              maxAttempts: 3
              interval: PT10S

            script: |
              import pandas as pd
              from kestra import Kestra

              print("Loading WEB data from Excel...")
              df = pd.read_excel('{{ vars.sourceWeb }}', dtype={'sku': str})

              print(f"Loaded {len(df)} rows")
              print("Sorting by total_sales (descending)...")
              df.sort_values(by='total_sales', ascending=False, inplace=True)

              print("Exporting to Parquet...")
              df.to_parquet('{{ vars.tempDir }}/web_raw.parquet', engine='fastparquet')

              Kestra.outputs({"total_raw": len(df)})

          # 2. CLEAN SOURCE
          - id: clean_web
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Remove rows with NULL sku"

            sql: |
              CREATE OR REPLACE TABLE web_clean AS
              SELECT *
              FROM read_parquet('{{ vars.tempDir }}/web_raw.parquet')
              WHERE sku IS NOT NULL;

              COPY web_clean TO '{{ vars.tempDir }}/web_clean.parquet' (FORMAT PARQUET);

              SELECT 
                COUNT(*) AS total_clean,
                {{ outputs.load_web.vars.total_raw }} - COUNT(*) AS cleaned
              FROM web_clean;

            fetchType: FETCH_ONE

          # 3. DEDUP CLEANED SOURCE
          - id: dedup_web
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Remove duplicate sku rows"

            sql: |-
              CREATE OR REPLACE TABLE dedup_web AS
              SELECT DISTINCT ON(sku) * 
              FROM read_parquet('{{ vars.tempDir }}/web_clean.parquet');

              COPY dedup_web TO '{{ vars.tempDir }}/dedup_web.parquet' (FORMAT PARQUET);

              SELECT 
                COUNT(*) AS total_dedup,
                {{ outputs.clean_web.outputs[0].row.total_clean }} - COUNT(*) AS dedup
              FROM dedup_web;

            fetchType: FETCH_ONE

          # 4. TEST FINAL DATA DB
          - id: test_web
            type: io.kestra.plugin.scripts.python.Script
            description: "Test WEB data quality metrics"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            dependencies:
              - duckdb
              - kestra
            retry:
              type: constant
              maxDuration: PT10S
              maxAttempts: 2
              interval: PT3S

            script: |
              import duckdb
              from kestra import Kestra

              dedup_file = "{{ vars.tempDir }}/dedup_web.parquet"

              # Test 1: Count missing sku
              sql = f"""
              SELECT COUNT(*) as total FROM read_parquet('{dedup_file}')
              WHERE sku IS NULL;
              """
              result = duckdb.sql(sql).fetchall()
              missings = result[0][0]
              print(f"Missing sku: {missings}")

              # Test 2: Count duplicate sku
              sql = f"""
              SELECT COUNT(*) as nb_sku_doublons
              FROM (
                SELECT sku
                FROM read_parquet('{dedup_file}')
                GROUP BY sku
                HAVING COUNT(*) > 1
              ) AS d
              """
              result = duckdb.sql(sql).fetchall()
              duplicated = result[0][0]
              print(f"Duplicated sku: {duplicated}")

              Kestra.outputs({"missings": missings, "duplicated": duplicated})

          # 5. â­ CRITICAL: Validate WEB Quality
          - id: validate_web_quality
            type: io.kestra.plugin.scripts.python.Script
            description: "â­ CRITICAL: Validate WEB quality - STOP pipeline if errors"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT10S
              maxAttempts: 2
              interval: PT3S

            script: |
              from kestra import Kestra

              missings = int("{{ outputs.test_web.vars.missings }}")
              duplicated = int("{{ outputs.test_web.vars.duplicated }}")

              print("=" * 60)
              print("ðŸ” WEB DATA QUALITY VALIDATION")
              print("=" * 60)
              print(f"  â€¢ Missing sku: {missings}")
              print(f"  â€¢ Duplicated sku: {duplicated}")

              MAX_MISSINGS = 0
              MAX_DUPLICATED = 0

              errors = []

              if missings > MAX_MISSINGS:
                  errors.append(f"CRITICAL: {missings} rows with missing sku detected")

              if duplicated > MAX_DUPLICATED:
                  errors.append(f"CRITICAL: {duplicated} duplicate sku values detected")

              if errors:
                  print("\n" + "=" * 60)
                  print("VALIDATION FAILED - STOPPING PIPELINE")
                  print("=" * 60)
                  for error in errors:
                      print(f"  {error}")
                  print("=" * 60)
                  raise Exception(f"WEB Quality Validation Failed: {len(errors)} critical issue(s)")

              print("\n" + "=" * 60)
              print("VALIDATION PASSED - WEB data quality OK")
              print("=" * 60)
              Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

          # 6. REPORT
          - id: log_web
            type: io.kestra.plugin.core.log.Log
            message: |
              ðŸ“Š â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                       WEB PIPELINE SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ðŸ“ Source: Fichier_web.xlsx
              ðŸ“ˆ Total rows loaded: {{ outputs.load_web.vars.total_raw }}
              ðŸ§¹ Rows cleaned: {{ outputs.clean_web.outputs[0].row.cleaned }}
              ðŸ”„ Rows after dedup: {{ outputs.dedup_web.outputs[0].row.total_dedup }}
              ðŸ§ª Missing sku: {{ outputs.test_web.vars.missings }}
              ðŸ§ª Duplicated sku: {{ outputs.test_web.vars.duplicated }}
              âœ… Quality check: {{ outputs.validate_web_quality.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      # =============== BRANCHE LIAISON ===============
      - id: process_link
        type: io.kestra.plugin.core.flow.Sequential
        description: "LIAISON data load"

        tasks:
          - id: load_link
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Load LIAISON linking file"

            sql: |-
              CREATE OR REPLACE TABLE link_raw AS
              SELECT *
              FROM read_xlsx('{{ vars.sourceLink }}', header = true);

              COPY link_raw TO '{{ vars.tempDir }}/link_raw.parquet' (FORMAT PARQUET);

              SELECT COUNT(*) as link_rows FROM link_raw;

            fetchType: FETCH_ONE

  # =============== MERGE & ANALYSES ===============
  - id: process_merge
    type: io.kestra.plugin.core.flow.Sequential
    description: "Merge and analyze data"

    tasks:
      - id: merge_files
        type: io.kestra.plugin.jdbc.duckdb.Queries
        description: "Merge ERP, WEB and LIAISON data"

        sql: |-
          CREATE OR REPLACE TABLE merged AS
          SELECT e.*, w.* FROM
            read_parquet('{{ vars.tempDir }}/dedup_erp.parquet') AS e
          INNER JOIN read_parquet('{{ vars.tempDir }}/link_raw.parquet') AS l
            ON e.product_id = l.product_id
          INNER JOIN read_parquet('{{ vars.tempDir }}/dedup_web.parquet') AS w
            ON l.id_web = w.sku;

          COPY merged TO '{{ vars.tempDir }}/merged.parquet' (FORMAT PARQUET);

          SELECT COUNT(*) as merged FROM merged;

        fetchType: FETCH_ONE

      - id: sales_total
        type: io.kestra.plugin.jdbc.duckdb.Query
        description: "Calculate total sales"

        sql: |
          SELECT ROUND(SUM(total_sales * price), 2) as sales_total
          FROM read_parquet('{{ vars.tempDir }}/merged.parquet');

      - id: sales_products
        type: io.kestra.plugin.jdbc.duckdb.Queries
        description: "Export sales by product to Excel"

        sql: |-
          INSTALL excel;
          LOAD excel;

          COPY (
            SELECT product_id, sku, post_title, total_sales, price,
                    ROUND(total_sales * price, 2) as sales_product
            FROM read_parquet('{{ vars.tempDir }}/merged.parquet')
            ORDER BY sales_product DESC
          ) TO '{{ vars.outputDir }}/sales_product.xlsx'
          WITH (FORMAT xlsx, HEADER true);

      - id: zscore
        type: io.kestra.plugin.scripts.python.Script
        description: "Calculate z-score for premium product detection"

        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        dependencies:
          - pandas
          - fastparquet

        script: |
          import pandas as pd

          df = pd.read_parquet('{{ vars.tempDir }}/merged.parquet')

          # Calculate z-score
          df['zscore'] = (df['price'] - df['price'].mean()) / df['price'].std()

          # Save to CSV
          df[['product_id', 'zscore']].to_csv('{{ vars.tempDir }}/zscore.csv', index=False)

      - id: process_class
        type: io.kestra.plugin.core.flow.Parallel
        description: "Export premium and ordinary products"

        tasks:
          - id: premium_csv
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Export premium wines (z-score > 2)"

            sql: |-
              COPY (
                SELECT z.zscore, m.* FROM
                  read_csv('{{ vars.tempDir }}/zscore.csv') as z
                INNER JOIN read_parquet('{{ vars.tempDir }}/merged.parquet') AS m
                  ON z.product_id = m.product_id
                WHERE z.zscore > 2
                ORDER BY zscore DESC
              ) TO '{{ vars.outputDir }}/premium.csv'
              WITH (FORMAT csv, HEADER true);

          - id: ordinary_csv
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Export ordinary wines (z-score <= 2)"

            sql: |-
              COPY (
                SELECT z.zscore, m.* FROM
                  read_csv('{{ vars.tempDir }}/zscore.csv') as z
                INNER JOIN read_parquet('{{ vars.tempDir }}/merged.parquet') AS m
                  ON z.product_id = m.product_id
                WHERE z.zscore <= 2
                ORDER BY zscore DESC
              ) TO '{{ vars.outputDir }}/ordinary.csv'
              WITH (FORMAT csv, HEADER true);

      - id: log_final
        type: io.kestra.plugin.core.log.Log
        message: |
          âœ… â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        PIPELINE EXECUTION COMPLETED
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          ðŸ“Š Merged records: {{ outputs.merge_files.outputs[0].row.merged }}
          ðŸ’° Total sales: {{ outputs.sales_total.row.sales_total }}

          ðŸ“‚ Output files:
            - sales_product.xlsx
            - premium.csv (z-score > 2)
            - ordinary.csv (z-score <= 2)

          âœ… All validations passed
          âœ… All exports completed
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
