id: ocde_p10
namespace: company.team

description: "P10 Wine Sales ETL Pipeline with Professional Error Handling"

variables:
  sourceErp: /app/data/sources/Fichier_erp.xlsx
  sourceWeb: /app/data/sources/Fichier_web.xlsx
  sourceLink: /app/data/sources/fichier_liaison.xlsx
  tempDir: /app/data/tmp
  outputDir: /app/data/output

tasks:
  - id: load_files
    type: io.kestra.plugin.core.flow.Parallel
    description: ETL Files in parallel
    tasks:
      - id: process_erp
        type: io.kestra.plugin.core.flow.Sequential
        description: "ERP extract â†’ clean â†’ dedup â†’ validate"

        tasks:
          # 1. LOAD SOURCE
          - id: load_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: Extract ERP data from Excel
            retry:
              type: constant
              maxAttempts: 3
              maxDuration: PT5M
              interval: PT3M

            sql: |-
              -- total lignes source
              CREATE OR REPLACE TABLE erp_raw AS
              SELECT *
              FROM read_xlsx('{{ vars.sourceErp }}', header = true);

              COPY erp_raw TO '{{ vars.tempDir }}/erp_raw.parquet' (FORMAT PARQUET);
              SELECT COUNT(*) AS total_raw FROM erp_raw;
            fetchType: FETCH_ONE

          # 2. CLEAN SOURCE 
          - id: clean_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: Remove missing data
            sql: |-
              CREATE OR REPLACE TABLE erp_clean AS
              SELECT *
              FROM read_parquet('{{ vars.tempDir }}/erp_raw.parquet')
              WHERE product_id IS NOT NULL;
              COPY erp_clean TO '{{ vars.tempDir }}/erp_clean.parquet' (FORMAT PARQUET);
              SELECT total_clean, {{ outputs.load_erp.outputs[0].row.total_raw }} - total_clean AS cleaned FROM ( SELECT COUNT(*) AS total_clean FROM erp_clean)
            fetchType: FETCH_ONE

          # 3. DEDUP CLEANED SOURCE
          - id: dedup_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Remove duplicate sku rows"

            sql: |-
              CREATE OR REPLACE TABLE dedup_erp AS
              SELECT DISTINCT ON(product_id) * FROM read_parquet('{{ vars.tempDir }}/erp_clean.parquet');
              COPY dedup_erp TO '{{ vars.tempDir }}/dedup_erp.parquet' (FORMAT PARQUET);
              SELECT total_dedup, {{ outputs.clean_erp.outputs[0].row.total_clean }} - total_dedup AS dedup FROM ( SELECT COUNT(*) AS total_dedup FROM dedup_erp)
            fetchType: FETCH_ONE

          # 4. TEST FINAL DATA DB
          - id: test_erp
            type: io.kestra.plugin.scripts.python.Script
            description: "Test ERP data quality metrics"
            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            dependencies:
              - duckdb
              - kestra
            script: |
              import duckdb
              from kestra import Kestra
              # Lire le fichier Excel
              dedup_file= "{{ vars.tempDir }}/erp_dedup.parquet"
              sql = f"""
              SELECT COUNT(*) as total FROM read_parquet('{dedup_file}')
              WHERE product_id IS NULL; 
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"missings": result[0][0]})

              sql = f"""
              SELECT COUNT(*) as nb_product_id_doublons
              FROM (
                  SELECT product_id
                  FROM read_parquet('{dedup_file}')
                  GROUP BY product_id
                  HAVING COUNT(*) > 1
              ) AS d   
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"duplicated": result[0][0]})

          # 5. â­ CRITICAL: Validate ERP Quality
          - id: validate_erp_quality
            type: io.kestra.plugin.scripts.python.Script
            description: "â­ CRITICAL: Validate ERP quality - STOP pipeline if errors"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            script: |
              from kestra import Kestra

              missings = int("{{ outputs.test_erp.vars.missings }}")
              duplicated = int("{{ outputs.test_erp.vars.duplicated }}")

              print("=" * 60)
              print("ğŸ” ERP DATA QUALITY VALIDATION")
              print("=" * 60)
              print(f"  â€¢ Missing product_id: {missings}")
              print(f"  â€¢ Duplicated product_id: {duplicated}")

              MAX_MISSINGS = 0
              MAX_DUPLICATED = 0

              errors = []

              if missings > MAX_MISSINGS:
                  errors.append(f"CRITICAL: {missings} rows with missing product_id detected")

              if duplicated > MAX_DUPLICATED:
                  errors.append(f"CRITICAL: {duplicated} duplicate product_id values detected")

              if errors:
                  print("\n" + "=" * 60)
                  print("VALIDATION FAILED - STOPPING PIPELINE")
                  print("=" * 60)
                  for error in errors:
                      print(f"  {error}")
                  print("=" * 60)
                  raise Exception(f"ERP Quality Validation Failed: {len(errors)} critical issue(s)")

              print("\n" + "=" * 60)
              print("VALIDATION PASSED - ERP data quality OK")
              print("=" * 60)
              Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

          # 6. REPORT PROCESS
          - id: log_erp
            type: io.kestra.plugin.core.log.Log
            description: Report erp.xlsx file ingestion
            message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         ERP PIPELINE SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: Fichier_erp.xlsx
              ğŸ“ˆ Total rows loaded: {{ outputs.load_erp.outputs[0].row.total_raw }}
              ğŸ§¹ Rows cleaned: {{ outputs.clean_erp.outputs[0].row.cleaned }}
              ğŸ”„ Rows after dedup: {{ outputs.dedup_erp.outputs[0].row.total_dedup }}
              ğŸ§ª Missing product_id: {{ outputs.test_erp.vars.missings }}
              ğŸ§ª Duplicated product_id: {{ outputs.test_erp.vars.duplicated }}
              âœ… Quality check: {{ outputs.validate_erp_quality.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - id: process_web
        type: io.kestra.plugin.core.flow.Sequential
        description: "WEB extract â†’ clean â†’ dedup â†’ validate"

        tasks:
          # 1. LOAD SOURCE
          - id: load_web
            type: io.kestra.plugin.scripts.python.Script
            description: "Load WEB data from Excel with retry"
            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT5M
              maxAttempts: 3
              interval: PT3M

            dependencies:
              - pandas
              - openpyxl
              - fastparquet
              - kestra

            script: |
              import pandas as pd
              from kestra import Kestra
              # Lire le fichier Excel
              df = pd.read_excel('{{ vars.sourceWeb }}', dtype = {'sku': str})
              df.sort_values(by='total_sales', ascending=False, inplace=True)
              # Sauvegarder en parquet
              df.to_parquet('{{ vars.tempDir }}/web_raw.parquet', engine='fastparquet')
              Kestra.outputs({"total_raw": len(df)})

          # 2. CLEAN SOURCE 
          - id: clean_web
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              CREATE OR REPLACE TABLE web_clean AS
              SELECT *
              FROM read_parquet('{{ vars.tempDir }}/web_raw.parquet')
              WHERE sku IS NOT NULL;
              COPY web_clean TO '{{ vars.tempDir }}/web_clean.parquet' (FORMAT PARQUET);
              SELECT total_clean, {{ outputs.load_web.vars.total_raw }} - total_clean AS cleaned FROM ( SELECT COUNT(*) AS total_clean FROM web_clean)
            fetchType: FETCH_ONE

          # 3. DEDUP CLEANED SOURCE
          - id: dedup_web
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              CREATE OR REPLACE TABLE dedup_web AS
              SELECT DISTINCT ON(sku) * FROM read_parquet('{{ vars.tempDir }}/web_clean.parquet');
              COPY dedup_web TO '{{ vars.tempDir }}/dedup_web.parquet' (FORMAT PARQUET);

              SELECT total_dedup, {{ outputs.clean_web.outputs[0].row.total_clean }} - total_dedup AS dedup FROM ( SELECT COUNT(*) AS total_dedup FROM dedup_web)
            fetchType: FETCH_ONE

          # 4. TEST FINAL DATA DB
          - id: test_web
            type: io.kestra.plugin.scripts.python.Script
            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            dependencies:
              - duckdb
              - kestra
            script: |
              import duckdb
              from kestra import Kestra
              # Lire le fichier Excel
              dedup_file= "{{ vars.tempDir }}/web_dedup.parquet"
              sql = f"""
              SELECT COUNT(*) as total FROM read_parquet('{dedup_file}')
              WHERE sku IS NULL; 
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"missings": result[0][0]})

              sql = f"""
              SELECT COUNT(*) as nb_sku_doublons
              FROM (
                  SELECT sku
                  FROM read_parquet('{dedup_file}')
                  GROUP BY sku
                  HAVING COUNT(*) > 1
              ) AS d   
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"duplicated": result[0][0]})

          # 5. â­ CRITICAL: Validate WEB Quality
          - id: validate_web_quality
            type: io.kestra.plugin.scripts.python.Script
            description: "â­ CRITICAL: Validate WEB quality - STOP pipeline if errors"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            script: |
              from kestra import Kestra

              missings = int("{{ outputs.test_web.vars.missings }}")
              duplicated = int("{{ outputs.test_web.vars.duplicated }}")

              print("=" * 60)
              print("ğŸ” WEB DATA QUALITY VALIDATION")
              print("=" * 60)
              print(f"  â€¢ Missing sku: {missings}")
              print(f"  â€¢ Duplicated sku: {duplicated}")

              MAX_MISSINGS = 0
              MAX_DUPLICATED = 0

              errors = []

              if missings > MAX_MISSINGS:
                  errors.append(f"CRITICAL: {missings} rows with missing sku detected")

              if duplicated > MAX_DUPLICATED:
                  errors.append(f"CRITICAL: {duplicated} duplicate sku values detected")

              if errors:
                  print("\n" + "=" * 60)
                  print("VALIDATION FAILED - STOPPING PIPELINE")
                  print("=" * 60)
                  for error in errors:
                      print(f"  {error}")
                  print("=" * 60)
                  raise Exception(f"WEB Quality Validation Failed: {len(errors)} critical issue(s)")

              print("\n" + "=" * 60)
              print("VALIDATION PASSED - WEB data quality OK")
              print("=" * 60)
              Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

          # 5. REPORT DEDUP
          - id: log_web
            type: io.kestra.plugin.core.log.Log
            message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         WEB PIPELINE SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: Fichier_web.xlsx
              ğŸ“ˆ Total rows loaded: {{ outputs.load_web.vars.total_raw }}
              ğŸ§¹ Rows cleaned: {{ outputs.clean_web.outputs[0].row.cleaned }}
              ğŸ”„ Rows after dedup: {{ outputs.dedup_web.outputs[0].row.total_dedup }}
              ğŸ§ª Missing sku: {{ outputs.test_web.vars.missings }}
              ğŸ§ª Duplicated sku: {{ outputs.test_web.vars.duplicated }}
              âœ… Quality check: {{ outputs.validate_web_quality.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      # =============== BRANCHE LIAISON ===============

      - id: process_link
        type: io.kestra.plugin.core.flow.Sequential
        tasks:
          # 1. LOAD SOURCE
          - id: load_link
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              -- total lignes source
              CREATE OR REPLACE TABLE web_raw AS
              SELECT *
              FROM read_xlsx('{{ vars.sourceLink }}', header = true, all_varchar = true);

              COPY web_raw TO '{{ vars.tempDir }}/link_raw.parquet' (FORMAT PARQUET);

  - id: process_merge
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: merge_files
        type: io.kestra.plugin.jdbc.duckdb.Queries
        sql: |-
          CREATE OR REPLACE TABLE merge AS
          SELECT e.*, w.* FROM read_parquet('{{ vars.tempDir }}/dedup_erp.parquet') AS e
          INNER JOIN read_parquet('{{ vars.tempDir }}/link_raw.parquet') AS l
           ON e.product_id = l.product_id
          INNER JOIN read_parquet('{{ vars.tempDir }}/dedup_web.parquet') AS w
           ON l.id_web = w.sku ;
          COPY merge TO '{{ vars.tempDir }}/merge.parquet' (FORMAT PARQUET);
          SELECT COUNT(*) as merged FROM merge;
        fetchType: FETCH_ONE

      - id: test_merge
        type: io.kestra.plugin.scripts.python.Script
        description: "Test ERP data quality metrics"
        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S

        dependencies:
          - duckdb
          - kestra
        script: |
          import duckdb
          from kestra import Kestra
    
          merge_file= "{{ vars.tempDir }}/merge.parquet"
          web_file="{{ vars.tempDir }}/web_dedup.parquet"
          sql = f"""
          SELECT COUNT(m.sku) as count FROM read_parquet('{web_file}') AS w
          LEFT JOIN read_parquet('{merge_file}') AS m ON w.sku = m.sku 
          WHERE m.sku IS NULL; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"web_lost": result[0][0]})

          erp_file="{{ vars.tempDir }}/erp_dedup.parquet"
          sql = f"""
          SELECT COUNT(m.product_id) as count FROM read_parquet('{merge_file}') AS m
          LEFT JOIN read_parquet('{erp_file}') AS e ON m.product_id = e.product_id
          WHERE m.product_id IS NULL; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"erp_lost": result[0][0]})


      # 5. â­ CRITICAL: Validate MERGE Quality
      - id: validate_merge
        type: io.kestra.plugin.scripts.python.Script
        description: "â­ CRITICAL: Validate MERGE quality - STOP pipeline if errors"

        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S

        script: |
          from kestra import Kestra

          web_lost = int("{{ outputs.test_merge.vars.web_lost }}")
          erp_lost = int("{{ outputs.test_merge.vars.erp_lost }}")

          print("=" * 60)
          print("ğŸ” MERGE DATA QUALITY VALIDATION")
          print("=" * 60)
          print(f"  â€¢ Missing web rows in merge file : {web_lost}")
          print(f"  â€¢ Missing erp rows in merge file : {erp_lost}")

          MAX_WEB_LOST = 0
          MAX_MISSING_PRODUCT = 0

          errors = []

          if web_lost > MAX_WEB_LOST:
              errors.append(f"CRITICAL: missing {web_lost} rows from web file in merge file")

          if erp_lost > MAX_MISSING_PRODUCT:
              errors.append(f"CRITICAL: missing {web_lost} rows from merge file in erp")

          if errors:
              print("\n" + "=" * 60)
              print("VALIDATION FAILED - STOPPING PIPELINE")
              print("=" * 60)
              for error in errors:
                  print(f"  {error}")
              print("=" * 60)
              raise Exception(f"MERGE Quality Validation Failed: {len(errors)} critical issue(s)")

          print("\n" + "=" * 60)
          print("VALIDATION PASSED - WEB data quality OK")
          print("=" * 60)
          Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

      - id: log_merge
        type: io.kestra.plugin.core.log.Log
        message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         MERGE PROCESS SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: dedup_web -- raw_link -- dedup_erp
              ğŸ”„ Rows in dedup_web : {{ outputs.dedup_web.outputs[0].row.total_dedup }}
              ğŸ“ˆ Total rows merged: {{ outputs.merge_files.outputs[0].row.merged }}
              ğŸ§ª Missing sku from web in merge : {{ outputs.test_merge.vars.web_lost }}
              ğŸ§ª Missing product from merge  in erp: {{ outputs.test_merge.vars.erp_lost }}
              âœ… Quality check: {{ outputs.validate_merge.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  - id: process_sales
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: sales_total
        type: io.kestra.plugin.jdbc.duckdb.Query
        sql: |-
          SELECT ROUND(SUM(total_sales * price),2 ) AS sales_total FROM '{{ vars.tempDir }}/merge.parquet';
        fetchType: FETCH_ONE

      - id: test_sales
        type: io.kestra.plugin.scripts.python.Script
        description: "Test SALES data quality metrics"
        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S
        dependencies:
          - duckdb
          - kestra
        script: |
          import duckdb
          from kestra import Kestra
    
          merge_file= "{{ vars.tempDir }}/merge.parquet"
          sql = f"""
          SELECT COUNT(m.sku) as count FROM read_parquet('{merge_file}') AS m 
          WHERE (m.price IS NULL OR m.price <=0 ) ; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"wrong_prices": result[0][0]})

          sql = f"""
          SELECT COUNT(m.sku) as count FROM read_parquet('{merge_file}') AS m 
          WHERE (m.total_sales IS NULL OR m.total_sales < 0) ; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"wrong_qty": result[0][0]})

      - id: validate_sales
        type: io.kestra.plugin.scripts.python.Script
        description: "â­ CRITICAL: Validate MERGE quality - STOP pipeline if errors"

        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S

        script: |
          from kestra import Kestra

          wrong_prices = int("{{ outputs.test_sales.vars.wrong_prices }}")
          wrong_qty = int("{{ outputs.test_sales.vars.wrong_qty }}")

          print("=" * 60)
          print("ğŸ” SALES DATA QUALITY VALIDATION")
          print("=" * 60)
          print(f"  â€¢ Missing/Negative/0 price or  in merge file : {wrong_prices}")
          print(f"  â€¢ Missing/Negative total_sales in merge file : {wrong_qty}")

          MAX_WRONG_PRICE = 0
          MAX_WRONG_QTY = 0

          errors = []

          if wrong_prices > MAX_WRONG_PRICE:
              errors.append(f"CRITICAL: {wrong_prices} missing /negative/ equal 0 prices in merge file")

          if wrong_qty > MAX_WRONG_QTY:
              errors.append(f"CRITICAL: {wrong_qty} missing /negative total_sales in merge file")

          if errors:
              print("\n" + "=" * 60)
              print("VALIDATION FAILED - STOPPING PIPELINE")
              print("=" * 60)
              for error in errors:
                  print(f"  {error}")
              print("=" * 60)
              raise Exception(f"Sales Computing  Quality Validation Failed: {len(errors)} critical issue(s)")

          print("\n" + "=" * 60)
          print("VALIDATION PASSED - Sales Computing  Quality OK")
          print("=" * 60)
          Kestra.outputs({"status": "SUCCESS", "validation_passed": True})
          
      - id: log_sales
        type: io.kestra.plugin.core.log.Log
        message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         MERGE PROCESS SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: merge file
              ğŸ“ˆ Incorrect prices : {{ outputs.test_sales.vars.wrong_prices }}
              ğŸ“ˆ Incorrect Sale quantity : {{ outputs.test_sales.vars.wrong_prices }}
              âœ… Quality check: {{ outputs.validate_sales.vars.status }}
              ğŸ§¹ Total sales: {{ outputs.sales_total.row.sales_total }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - id: sales_products
        type: io.kestra.plugin.jdbc.duckdb.Queries
        sql: |-
          INSTALL excel;
          LOAD excel;
          COPY(SELECT product_id, sku, post_title, onsale_web, stock_quantity , total_sales, price, ROUND(total_sales * price , 2) AS sales_product FROM read_parquet('{{ vars.tempDir }}/merge.parquet') ORDER BY sales_product DESC)
           TO '{{ vars.outputDir }}/sales_product.xlsx ' WITH(FORMAT xlsx, HEADER true);

      - id: z_score
        type: io.kestra.plugin.scripts.python.Script
        taskRunner:
          type: io.kestra.plugin.core.runner.Process
        dependencies:
          - pandas
          - fastparquet
        script: |-
          import pandas as pd
          df = pd.read_parquet('{{ vars.tempDir }}/merge.parquet')
          df['z_score']= (df.price - df.price.mean())/df.price.std()
          df[['product_id', 'z_score']].to_csv('{{ vars.tempDir }}/z_score.csv')

  - id: process_class
    type: io.kestra.plugin.core.flow.Parallel
    description: Export CSV
    tasks:

      - id: premium_csv
        type: io.kestra.plugin.jdbc.duckdb.Queries
        sql: |-
          COPY(SELECT z.z_score, m.* FROM read_csv('{{ vars.tempDir }}/z_score.csv') as z 
          INNER JOIN read_parquet('{{ vars.tempDir }}/merge.parquet') AS m ON z.product_id = m.product_id 
          WHERE z.z_score >=2 ORDER BY z_score DESC )
           TO '{{ vars.outputDir }}/premium.csv ' WITH(FORMAT csv, HEADER true);

      - id: ordinary_csv
        type: io.kestra.plugin.jdbc.duckdb.Queries
        sql: |-
          COPY(SELECT z.z_score, m.* FROM read_csv('{{ vars.tempDir }}/z_score.csv') as z 
          INNER JOIN read_parquet('{{ vars.tempDir }}/merge.parquet') AS m ON z.product_id = m.product_id 
          WHERE z.z_score < 2 ORDER BY z_score DESC )
           TO '{{ vars.outputDir }}/ordinary.csv ' WITH(FORMAT csv, HEADER true);

  - id: log_final
    type: io.kestra.plugin.core.log.Log
    message: |+
      -
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      ğŸ“Š              PIPELINE EXECUTION COMPLETED
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      ğŸ“Š Merged records: {{ outputs.merge_files.outputs[0].row.merged }}
      ğŸ’° Total sales: {{ outputs.sales_total.row.sales_total }}

      ğŸ“‚ Output files:
        - sales_product.xlsx
        - premium.csv (z-score > 2)
        - ordinary.csv (z-score <= 2)

      âœ… All validations passed
      âœ… All exports completed
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


        
triggers:
  - id: monthly_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: 0 9 15 * *
