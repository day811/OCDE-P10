id: ocde_p10
namespace: company.team

description: "P10 Wine Sales ETL Pipeline with Professional Error Handling"

variables:
  sourceErp: /app/data/sources/Fichier_erp.xlsx
  sourceWeb: /app/data/sources/Fichier_web.xlsx
  sourceLink: /app/data/sources/fichier_liaison.xlsx
  tempDir: /app/data/tmp
  outputDir: /app/data/output

tasks:
  - id: load_files
    type: io.kestra.plugin.core.flow.Parallel
    description: ETL Files in parallel
    tasks:
      - id: process_erp
        type: io.kestra.plugin.core.flow.Sequential
        description: "ERP extract â†’ clean â†’ dedup â†’ validate"

        tasks:
          # 1. LOAD SOURCE
          - id: load_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: Extract ERP data from Excel
            retry:
              type: constant
              maxAttempts: 3
              maxDuration: PT5M
              interval: PT3M

            sql: |-
              -- total lignes source
              CREATE OR REPLACE TABLE erp_raw AS
              SELECT *
              FROM read_xlsx('{{ vars.sourceErp }}', header = true);

              COPY erp_raw TO '{{ vars.tempDir }}/erp_raw.parquet' (FORMAT PARQUET);
              SELECT COUNT(*) AS total_raw FROM erp_raw;
            fetchType: FETCH_ONE

          # 2. CLEAN SOURCE 
          - id: clean_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: Remove missing data
            sql: |-
              CREATE OR REPLACE TABLE erp_clean AS
              SELECT *
              FROM read_parquet('{{ vars.tempDir }}/erp_raw.parquet')
              WHERE product_id IS NOT NULL;
              COPY erp_clean TO '{{ vars.tempDir }}/erp_clean.parquet' (FORMAT PARQUET);
              SELECT total_clean, {{ outputs.load_erp.outputs[0].row.total_raw }} - total_clean AS cleaned FROM ( SELECT COUNT(*) AS total_clean FROM erp_clean)
            fetchType: FETCH_ONE

          # 3. DEDUP CLEANED SOURCE
          - id: dedup_erp
            type: io.kestra.plugin.jdbc.duckdb.Queries
            description: "Remove duplicate sku rows"

            sql: |-
              CREATE OR REPLACE TABLE dedup_erp AS
              SELECT DISTINCT ON(product_id) * FROM read_parquet('{{ vars.tempDir }}/erp_clean.parquet');
              COPY dedup_erp TO '{{ vars.tempDir }}/dedup_erp.parquet' (FORMAT PARQUET);
              SELECT total_dedup, {{ outputs.clean_erp.outputs[0].row.total_clean }} - total_dedup AS dedup FROM ( SELECT COUNT(*) AS total_dedup FROM dedup_erp)
            fetchType: FETCH_ONE

          # 4. TEST FINAL DATA DB
          - id: test_erp
            type: io.kestra.plugin.scripts.python.Script
            description: "Test ERP data quality metrics"
            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            dependencies:
              - duckdb
              - kestra
            script: |
              import duckdb
              from kestra import Kestra
              # Lire le fichier Excel
              dedup_file= "{{ vars.tempDir }}/erp_dedup.parquet"
              sql = f"""
              SELECT COUNT(*) as total FROM read_parquet('{dedup_file}')
              WHERE product_id IS NULL; 
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"missings": result[0][0]})

              sql = f"""
              SELECT COUNT(*) as nb_product_id_doublons
              FROM (
                  SELECT product_id
                  FROM read_parquet('{dedup_file}')
                  GROUP BY product_id
                  HAVING COUNT(*) > 1
              ) AS d   
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"duplicated": result[0][0]})

          # 5. â­ CRITICAL: Validate ERP Quality
          - id: validate_erp_quality
            type: io.kestra.plugin.scripts.python.Script
            description: "â­ CRITICAL: Validate ERP quality - STOP pipeline if errors"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            script: |
              from kestra import Kestra

              missings = int("{{ outputs.test_erp.vars.missings }}")
              duplicated = int("{{ outputs.test_erp.vars.duplicated }}")

              print("=" * 60)
              print("ğŸ” ERP DATA QUALITY VALIDATION")
              print("=" * 60)
              print(f"  â€¢ Missing product_id: {missings}")
              print(f"  â€¢ Duplicated product_id: {duplicated}")

              MAX_MISSINGS = 0
              MAX_DUPLICATED = 0

              errors = []

              if missings > MAX_MISSINGS:
                  errors.append(f"CRITICAL: {missings} rows with missing product_id detected")

              if duplicated > MAX_DUPLICATED:
                  errors.append(f"CRITICAL: {duplicated} duplicate product_id values detected")

              if errors:
                  print("\n" + "=" * 60)
                  print("VALIDATION FAILED - STOPPING PIPELINE")
                  print("=" * 60)
                  for error in errors:
                      print(f"  {error}")
                  print("=" * 60)
                  raise Exception(f"ERP Quality Validation Failed: {len(errors)} critical issue(s)")

              print("\n" + "=" * 60)
              print("VALIDATION PASSED - ERP data quality OK")
              print("=" * 60)
              Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

          # 6. REPORT PROCESS
          - id: log_erp
            type: io.kestra.plugin.core.log.Log
            description: Report erp.xlsx file ingestion
            message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         ERP PIPELINE SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: Fichier_erp.xlsx
              ğŸ“ˆ Total rows loaded: {{ outputs.load_erp.outputs[0].row.total_raw }}
              ğŸ§¹ Rows cleaned: {{ outputs.clean_erp.outputs[0].row.cleaned }}
              ğŸ”„ Rows after dedup: {{ outputs.dedup_erp.outputs[0].row.total_dedup }}
              ğŸ§ª Missing product_id: {{ outputs.test_erp.vars.missings }}
              ğŸ§ª Duplicated product_id: {{ outputs.test_erp.vars.duplicated }}
              âœ… Quality check: {{ outputs.validate_erp_quality.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - id: process_web
        type: io.kestra.plugin.core.flow.Sequential
        description: "WEB extract â†’ clean â†’ dedup â†’ validate"

        tasks:
          # 1. LOAD SOURCE
          - id: load_web
            type: io.kestra.plugin.scripts.python.Script
            description: "Load WEB data from Excel with retry"
            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT5M
              maxAttempts: 3
              interval: PT3M

            dependencies:
              - pandas
              - openpyxl
              - fastparquet
              - kestra

            script: |
              import pandas as pd
              from kestra import Kestra
              # Lire le fichier Excel
              df = pd.read_excel('{{ vars.sourceWeb }}', dtype = {'sku': str})
              df.sort_values(by='total_sales', ascending=False, inplace=True)
              # Sauvegarder en parquet
              df.to_parquet('{{ vars.tempDir }}/web_raw.parquet', engine='fastparquet')
              Kestra.outputs({"total_raw": len(df)})

          # 2. CLEAN SOURCE 
          - id: clean_web
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              CREATE OR REPLACE TABLE web_clean AS
              SELECT *
              FROM read_parquet('{{ vars.tempDir }}/web_raw.parquet')
              WHERE sku IS NOT NULL;
              COPY web_clean TO '{{ vars.tempDir }}/web_clean.parquet' (FORMAT PARQUET);
              SELECT total_clean, {{ outputs.load_web.vars.total_raw }} - total_clean AS cleaned FROM ( SELECT COUNT(*) AS total_clean FROM web_clean)
            fetchType: FETCH_ONE

          # 3. DEDUP CLEANED SOURCE
          - id: dedup_web
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              CREATE OR REPLACE TABLE dedup_web AS
              SELECT DISTINCT ON(sku) * FROM read_parquet('{{ vars.tempDir }}/web_clean.parquet');
              COPY dedup_web TO '{{ vars.tempDir }}/dedup_web.parquet' (FORMAT PARQUET);

              SELECT total_dedup, {{ outputs.clean_web.outputs[0].row.total_clean }} - total_dedup AS dedup FROM ( SELECT COUNT(*) AS total_dedup FROM dedup_web)
            fetchType: FETCH_ONE

          # 4. TEST FINAL DATA DB
          - id: test_web
            type: io.kestra.plugin.scripts.python.Script
            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            dependencies:
              - duckdb
              - kestra
            script: |
              import duckdb
              from kestra import Kestra
              # Lire le fichier Excel
              dedup_file= "{{ vars.tempDir }}/web_dedup.parquet"
              sql = f"""
              SELECT COUNT(*) as total FROM read_parquet('{dedup_file}')
              WHERE sku IS NULL; 
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"missings": result[0][0]})

              sql = f"""
              SELECT COUNT(*) as nb_sku_doublons
              FROM (
                  SELECT sku
                  FROM read_parquet('{dedup_file}')
                  GROUP BY sku
                  HAVING COUNT(*) > 1
              ) AS d   
              """
              result = duckdb.sql(sql).fetchall()
              print(result[0][0])
              Kestra.outputs({"duplicated": result[0][0]})

          # 5. â­ CRITICAL: Validate WEB Quality
          - id: validate_web_quality
            type: io.kestra.plugin.scripts.python.Script
            description: "â­ CRITICAL: Validate WEB quality - STOP pipeline if errors"

            taskRunner:
              type: io.kestra.plugin.core.runner.Process

            retry:
              type: constant
              maxDuration: PT3M
              maxAttempts: 2
              interval: PT30S

            script: |
              from kestra import Kestra

              missings = int("{{ outputs.test_web.vars.missings }}")
              duplicated = int("{{ outputs.test_web.vars.duplicated }}")

              print("=" * 60)
              print("ğŸ” WEB DATA QUALITY VALIDATION")
              print("=" * 60)
              print(f"  â€¢ Missing sku: {missings}")
              print(f"  â€¢ Duplicated sku: {duplicated}")

              MAX_MISSINGS = 0
              MAX_DUPLICATED = 0

              errors = []

              if missings > MAX_MISSINGS:
                  errors.append(f"CRITICAL: {missings} rows with missing sku detected")

              if duplicated > MAX_DUPLICATED:
                  errors.append(f"CRITICAL: {duplicated} duplicate sku values detected")

              if errors:
                  print("\n" + "=" * 60)
                  print("VALIDATION FAILED - STOPPING PIPELINE")
                  print("=" * 60)
                  for error in errors:
                      print(f"  {error}")
                  print("=" * 60)
                  raise Exception(f"WEB Quality Validation Failed: {len(errors)} critical issue(s)")

              print("\n" + "=" * 60)
              print("VALIDATION PASSED - WEB data quality OK")
              print("=" * 60)
              Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

          # 5. REPORT DEDUP
          - id: log_web
            type: io.kestra.plugin.core.log.Log
            message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         WEB PIPELINE SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: Fichier_web.xlsx
              ğŸ“ˆ Total rows loaded: {{ outputs.load_web.vars.total_raw }}
              ğŸ§¹ Rows cleaned: {{ outputs.clean_web.outputs[0].row.cleaned }}
              ğŸ”„ Rows after dedup: {{ outputs.dedup_web.outputs[0].row.total_dedup }}
              ğŸ§ª Missing sku: {{ outputs.test_web.vars.missings }}
              ğŸ§ª Duplicated sku: {{ outputs.test_web.vars.duplicated }}
              âœ… Quality check: {{ outputs.validate_web_quality.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      # =============== BRANCHE LIAISON ===============

      - id: process_link
        type: io.kestra.plugin.core.flow.Sequential
        tasks:
          # 1. LOAD SOURCE
          - id: load_link
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              -- total lignes source
              CREATE OR REPLACE TABLE web_raw AS
              SELECT *
              FROM read_xlsx('{{ vars.sourceLink }}', header = true, all_varchar = true);

              COPY web_raw TO '{{ vars.tempDir }}/link_raw.parquet' (FORMAT PARQUET);

  - id: process_merge
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: merge_files
        type: io.kestra.plugin.jdbc.duckdb.Queries
        sql: |-
          CREATE OR REPLACE TABLE merge AS
          SELECT e.*, w.* FROM read_parquet('{{ vars.tempDir }}/dedup_erp.parquet') AS e
          INNER JOIN read_parquet('{{ vars.tempDir }}/link_raw.parquet') AS l
           ON e.product_id = l.product_id
          INNER JOIN read_parquet('{{ vars.tempDir }}/dedup_web.parquet') AS w
           ON l.id_web = w.sku ;
          COPY merge TO '{{ vars.tempDir }}/merge.parquet' (FORMAT PARQUET);
          SELECT COUNT(*) as merged FROM merge;
        fetchType: FETCH_ONE

      - id: test_merge
        type: io.kestra.plugin.scripts.python.Script
        description: "Test ERP data quality metrics"
        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S

        dependencies:
          - duckdb
          - kestra
        script: |
          import duckdb
          from kestra import Kestra
    
          merge_file= "{{ vars.tempDir }}/merge.parquet"
          web_file="{{ vars.tempDir }}/web_dedup.parquet"
          sql = f"""
          SELECT COUNT(m.sku) as count FROM read_parquet('{web_file}') AS w
          LEFT JOIN read_parquet('{merge_file}') AS m ON w.sku = m.sku 
          WHERE m.sku IS NULL; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"web_lost": result[0][0]})

          erp_file="{{ vars.tempDir }}/erp_dedup.parquet"
          sql = f"""
          SELECT COUNT(m.product_id) as count FROM read_parquet('{merge_file}') AS m
          LEFT JOIN read_parquet('{erp_file}') AS e ON m.product_id = e.product_id
          WHERE m.product_id IS NULL; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"erp_lost": result[0][0]})


      # 5. â­ CRITICAL: Validate MERGE Quality
      - id: validate_merge
        type: io.kestra.plugin.scripts.python.Script
        description: "â­ CRITICAL: Validate MERGE quality - STOP pipeline if errors"

        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S

        script: |
          from kestra import Kestra

          web_lost = int("{{ outputs.test_merge.vars.web_lost }}")
          erp_lost = int("{{ outputs.test_merge.vars.erp_lost }}")

          print("=" * 60)
          print("ğŸ” MERGE DATA QUALITY VALIDATION")
          print("=" * 60)
          print(f"  â€¢ Missing web rows in merge file : {web_lost}")
          print(f"  â€¢ Missing erp rows in merge file : {erp_lost}")

          MAX_WEB_LOST = 0
          MAX_MISSING_PRODUCT = 0

          errors = []

          if web_lost > MAX_WEB_LOST:
              errors.append(f"CRITICAL: missing {web_lost} rows from web file in merge file")

          if erp_lost > MAX_MISSING_PRODUCT:
              errors.append(f"CRITICAL: missing {web_lost} rows from merge file in erp")

          if errors:
              print("\n" + "=" * 60)
              print("VALIDATION FAILED - STOPPING PIPELINE")
              print("=" * 60)
              for error in errors:
                  print(f"  {error}")
              print("=" * 60)
              raise Exception(f"MERGE Quality Validation Failed: {len(errors)} critical issue(s)")

          print("\n" + "=" * 60)
          print("VALIDATION PASSED - WEB data quality OK")
          print("=" * 60)
          Kestra.outputs({"status": "SUCCESS", "validation_passed": True})

      - id: log_merge
        type: io.kestra.plugin.core.log.Log
        message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         MERGE PROCESS SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: dedup_web -- raw_link -- dedup_erp
              ğŸ”„ Rows in dedup_web : {{ outputs.dedup_web.outputs[0].row.total_dedup }}
              ğŸ“ˆ Total rows merged: {{ outputs.merge_files.outputs[0].row.merged }}
              ğŸ§ª Missing sku from web in merge : {{ outputs.test_merge.vars.web_lost }}
              ğŸ§ª Missing product from merge  in erp: {{ outputs.test_merge.vars.erp_lost }}
              âœ… Quality check: {{ outputs.validate_merge.vars.status }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  - id: process_sales
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: sales_total
        type: io.kestra.plugin.jdbc.duckdb.Query
        sql: |-
          SELECT ROUND(SUM(total_sales * price),2 ) AS sales_total FROM '{{ vars.tempDir }}/merge.parquet';
        fetchType: FETCH_ONE

      - id: test_sales
        type: io.kestra.plugin.scripts.python.Script
        description: "Test SALES data quality metrics"
        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S
        dependencies:
          - duckdb
          - kestra
        script: |
          import duckdb
          from kestra import Kestra
    
          merge_file= "{{ vars.tempDir }}/merge.parquet"
          sql = f"""
          SELECT COUNT(m.sku) as count FROM read_parquet('{merge_file}') AS m 
          WHERE (m.price IS NULL OR m.price <=0 ) ; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"wrong_prices": result[0][0]})

          sql = f"""
          SELECT COUNT(m.sku) as count FROM read_parquet('{merge_file}') AS m 
          WHERE (m.total_sales IS NULL OR m.total_sales < 0) ; 
          """
          result = duckdb.sql(sql).fetchall()
          print(result[0][0])
          Kestra.outputs({"wrong_qty": result[0][0]})

      - id: validate_sales
        type: io.kestra.plugin.scripts.python.Script
        description: "â­ CRITICAL: Validate MERGE quality - STOP pipeline if errors"

        taskRunner:
          type: io.kestra.plugin.core.runner.Process

        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S

        script: |
          from kestra import Kestra

          wrong_prices = int("{{ outputs.test_sales.vars.wrong_prices }}")
          wrong_qty = int("{{ outputs.test_sales.vars.wrong_qty }}")

          print("=" * 60)
          print("ğŸ” SALES DATA QUALITY VALIDATION")
          print("=" * 60)
          print(f"  â€¢ Missing/Negative/0 price or  in merge file : {wrong_prices}")
          print(f"  â€¢ Missing/Negative total_sales in merge file : {wrong_qty}")

          MAX_WRONG_PRICE = 0
          MAX_WRONG_QTY = 0

          errors = []

          if wrong_prices > MAX_WRONG_PRICE:
              errors.append(f"CRITICAL: {wrong_prices} missing /negative/ equal 0 prices in merge file")

          if wrong_qty > MAX_WRONG_QTY:
              errors.append(f"CRITICAL: {wrong_qty} missing /negative total_sales in merge file")

          if errors:
              print("\n" + "=" * 60)
              print("VALIDATION FAILED - STOPPING PIPELINE")
              print("=" * 60)
              for error in errors:
                  print(f"  {error}")
              print("=" * 60)
              raise Exception(f"Sales Computing  Quality Validation Failed: {len(errors)} critical issue(s)")

          print("\n" + "=" * 60)
          print("VALIDATION PASSED - Sales Computing  Quality OK")
          print("=" * 60)
          Kestra.outputs({"status": "SUCCESS", "validation_passed": True})
          
      - id: log_sales
        type: io.kestra.plugin.core.log.Log
        message: |
              -
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“Š         MERGE PROCESS SUMMARY
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
              ğŸ“ Source: merge file
              ğŸ“ˆ Incorrect prices : {{ outputs.test_sales.vars.wrong_prices }}
              ğŸ“ˆ Incorrect Sale quantity : {{ outputs.test_sales.vars.wrong_prices }}
              âœ… Quality check: {{ outputs.validate_sales.vars.status }}
              ğŸ§¹ Total sales: {{ outputs.sales_total.row.sales_total }}
              â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      - id: sales_products
        type: io.kestra.plugin.jdbc.duckdb.Queries
        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S

        sql: |-
          INSTALL excel;
          LOAD excel;
          COPY(SELECT product_id, sku, post_title, onsale_web, stock_quantity , total_sales, price, ROUND(total_sales * price , 2) AS sales_product FROM read_parquet('{{ vars.tempDir }}/merge.parquet') ORDER BY sales_product DESC)
           TO '{{ vars.outputDir }}/sales_product.xlsx ' WITH(FORMAT xlsx, HEADER true);

  - id: process_zscore
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: z_score
        type: io.kestra.plugin.scripts.python.Script
        taskRunner:
          type: io.kestra.plugin.core.runner.Process
        dependencies:
          - pandas
          - fastparquet
        script: |-
          import pandas as pd
          df = pd.read_parquet('{{ vars.tempDir }}/merge.parquet')
          df['z_score']= (df.price - df.price.mean())/df.price.std()
          df[['product_id', 'z_score']].to_csv('{{ vars.tempDir }}/z_score.csv')

      - id: test_zscore
        type: io.kestra.plugin.scripts.python.Script
        description: "Test Z-score calculation consistency and segmentation"
        
        taskRunner:
          type: io.kestra.plugin.core.runner.Process
        
        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S
        
        dependencies:
          - duckdb
          - pandas
          - kestra
          - fastparquet
        
        script: |
          import duckdb
          import pandas as pd
          from kestra import Kestra
          
          merge_file = "{{ vars.tempDir }}/merge.parquet"
          zscore_file = "{{ vars.tempDir }}/z_score.csv"
          
          # Read merge and z_score data
          df_merge = pd.read_parquet(merge_file, engine='fastparquet')
          df_zscore = pd.read_csv(zscore_file)
          
          # Test 1: Count total products in both files
          sql = f"""
          SELECT COUNT(*) as total_merge FROM read_parquet('{merge_file}')
          """
          total_merge = duckdb.sql(sql).fetchall()[0][0]
          total_zscore = len(df_zscore)
          
          # Test 2: Count premium (z_score >= 2)
          premium_count = (df_zscore['z_score'] >= 2).sum()
          
          # Test 3: Count ordinary (z_score < 2)
          ordinary_count = (df_zscore['z_score'] < 2).sum()
          
          # Test 4: Check z_score statistics
          z_mean = df_zscore['z_score'].mean()
          z_std = df_zscore['z_score'].std()
          z_min = df_zscore['z_score'].min()
          z_max = df_zscore['z_score'].max()
          
          # Test 5: Verify segmentation consistency
          consistency = (premium_count + ordinary_count) == total_zscore
          
          # Test 6: Verify no missing z_score values
          missing_zscore = df_zscore['z_score'].isnull().sum()
          
          print(f"""
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          ğŸ§® Z-SCORE CALCULATION TEST
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

          ğŸ“Š DATA COVERAGE:
            â€¢ Total products in merge: {total_merge}
            â€¢ Total z_scores calculated: {total_zscore}
            â€¢ Match: {'âœ… YES' if total_merge == total_zscore else 'âŒ NO'}

          ğŸ† SEGMENTATION:
            â€¢ Premium (z_score â‰¥ 2): {premium_count}
            â€¢ Ordinary (z_score < 2): {ordinary_count}
            â€¢ Total: {premium_count + ordinary_count}
            â€¢ Consistency: {'âœ… OK' if consistency else 'âŒ FAIL'}

          ğŸ“ˆ Z-SCORE STATISTICS:
            â€¢ Mean: {z_mean:.4f}
            â€¢ Std Dev: {z_std:.4f}
            â€¢ Min: {z_min:.4f}
            â€¢ Max: {z_max:.4f}

          âš ï¸ DATA QUALITY:
            â€¢ Missing z_score values: {missing_zscore}

          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          """)
          
          # Output results for validation
          Kestra.outputs({
              "total_merge": int(total_merge),
              "total_zscore": int(total_zscore),
              "premium_count": int(premium_count),
              "ordinary_count": int(ordinary_count),
              "z_score_mean": float(z_mean),
              "z_score_std": float(z_std),
              "z_score_min": float(z_min),
              "z_score_max": float(z_max),
              "consistency": int(consistency),
              "missing_zscore": int(missing_zscore)
          })

      - id: validate_zscore
        type: io.kestra.plugin.scripts.python.Script
        description: "â­ CRITICAL: Validate Z-score consistency - STOP pipeline if errors"
        
        taskRunner:
          type: io.kestra.plugin.core.runner.Process
        
        retry:
          type: constant
          maxDuration: PT3M
          maxAttempts: 2
          interval: PT30S
        
        script: |
          from kestra import Kestra
          
          total_merge = int("{{ outputs.test_zscore.vars.total_merge }}")
          total_zscore = int("{{ outputs.test_zscore.vars.total_zscore }}")
          premium_count = int("{{ outputs.test_zscore.vars.premium_count }}")
          ordinary_count = int("{{ outputs.test_zscore.vars.ordinary_count }}")
          consistency = {{ outputs.test_zscore.vars.consistency }}
          missing_zscore = int("{{ outputs.test_zscore.vars.missing_zscore }}")
          z_score_mean = float("{{ outputs.test_zscore.vars.z_score_mean }}")
          z_score_std = float("{{ outputs.test_zscore.vars.z_score_std }}")
          
          print("=" * 60)
          print("ğŸ” Z-SCORE VALIDATION")
          print("=" * 60)
          
          errors = []
          warnings = []
          
          # CRITICAL: Data coverage mismatch
          if total_merge != total_zscore:
              errors.append(
                  f"CRITICAL: Merge has {total_merge} rows but z_score has {total_zscore}"
              )
          
          # CRITICAL: Segmentation inconsistency
          if not consistency:
              errors.append(
                  f"CRITICAL: Segmentation failed (premium + ordinary != total)"
              )
          
          # CRITICAL: Missing z_score values
          if missing_zscore > 0:
              errors.append(
                  f"CRITICAL: {missing_zscore} missing z_score values"
              )
          
          # WARNING: No premium products detected
          if premium_count == 0:
              warnings.append(
                  f"âš ï¸ WARNING: No premium products detected (z_score â‰¥ 2)"
              )
          
          # WARNING: Unusual z_score distribution
          if z_score_std < 0.1:
              warnings.append(
                  f"âš ï¸ WARNING: Z-score std dev very low ({z_score_std:.4f}) - prices may lack variance"
              )
          
          # Display errors
          if errors:
              print("\nğŸ”´ VALIDATION FAILED:")
              for err in errors:
                  print(f"  â€¢ {err}")
              print("=" * 60)
              raise Exception(f"Z-Score Validation Failed: {len(errors)} critical issue(s)")
          
          print("\nâœ… VALIDATION PASSED")
          
          # Display warnings
          if warnings:
              print("\nâš ï¸ WARNINGS:")
              for warn in warnings:
                  print(f"  {warn}")
              status = "SUCCESS_WITH_WARNINGS"
          else:
              print("   â€¢ Segmentation coherent")
              print("   â€¢ All z_scores calculated")
              status = "SUCCESS"
          
          print("=" * 60)
          
          Kestra.outputs({
              "status": status,
              "validation_passed": True,
              "premium_count": premium_count,
              "ordinary_count": ordinary_count
          })

      - id: log_zscore
        type: io.kestra.plugin.core.log.Log
        description: "Report Z-score calculation and segmentation summary"
        
        message: |
          -
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          
          ğŸ† Z-SCORE ANALYSIS SUMMARY
          
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          
          ğŸ“Š CALCULATION:
            â€¢ Z-score mean: {{ outputs.test_zscore.vars.z_score_mean }}
            â€¢ Z-score std dev: {{ outputs.test_zscore.vars.z_score_std }}
            â€¢ Z-score range: [{{ outputs.test_zscore.vars.z_score_min }}, {{ outputs.test_zscore.vars.z_score_max }}]
          
          ğŸ¯ SEGMENTATION:
            â€¢ Premium wines (z_score â‰¥ 2): {{ outputs.validate_zscore.vars.premium_count }}
            â€¢ Ordinary wines (z_score < 2): {{ outputs.validate_zscore.vars.ordinary_count }}
            â€¢ Total: {{ outputs.test_zscore.vars.total_zscore }}
          
          âœ… Quality check: {{ outputs.validate_zscore.vars.status }}
          
          ğŸ“ Output files will be created:
            â€¢ premium.csv ({{ outputs.validate_zscore.vars.premium_count }} products)
            â€¢ ordinary.csv ({{ outputs.validate_zscore.vars.ordinary_count }} products)
          
          â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


      - id: process_class
        type: io.kestra.plugin.core.flow.Parallel
        description: Export CSV
        tasks:

          - id: premium_csv
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              COPY(SELECT z.z_score, m.* FROM read_csv('{{ vars.tempDir }}/z_score.csv') as z 
              INNER JOIN read_parquet('{{ vars.tempDir }}/merge.parquet') AS m ON z.product_id = m.product_id 
              WHERE z.z_score >=2 ORDER BY z_score DESC )
              TO '{{ vars.outputDir }}/premium.csv ' WITH(FORMAT csv, HEADER true);

          - id: ordinary_csv
            type: io.kestra.plugin.jdbc.duckdb.Queries
            sql: |-
              COPY(SELECT z.z_score, m.* FROM read_csv('{{ vars.tempDir }}/z_score.csv') as z 
              INNER JOIN read_parquet('{{ vars.tempDir }}/merge.parquet') AS m ON z.product_id = m.product_id 
              WHERE z.z_score < 2 ORDER BY z_score DESC )
              TO '{{ vars.outputDir }}/ordinary.csv ' WITH(FORMAT csv, HEADER true);

  - id: log_final
    type: io.kestra.plugin.core.log.Log
    message: |+
      -
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      ğŸ“Š              PIPELINE EXECUTION COMPLETED
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      ğŸ“Š Merged records: {{ outputs.merge_files.outputs[0].row.merged }}
      ğŸ’° Total sales: {{ outputs.sales_total.row.sales_total }}

      ğŸ“‚ Output files:
        - sales_product.xlsx
        - premium.csv (z-score > 2)
        - ordinary.csv (z-score <= 2)

      âœ… All validations passed
      âœ… All exports completed
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


        
triggers:
  - id: monthly_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: 0 9 15 * *
